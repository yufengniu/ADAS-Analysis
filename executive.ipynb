{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "skill = pd.read_csv('skills-2.csv')\n",
    "achievement = pd.read_csv('achievements-2.csv')\n",
    "role_2 = pd.read_csv('roles-2.csv')\n",
    "occupation_group = pd.read_csv('occupation_group.csv')\n",
    "role_2 = role_2.merge(occupation_group, left_on='occupation', right_on='occupation_name', how='left')\n",
    "final_id = pd.read_csv('target_employees-2.csv')\n",
    "filtered_role = role_2[role_2['anon_user_id'].isin(final_id['anon_user_id'])]\n",
    "filtered_role = filtered_role[filtered_role['role_end_date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_role.anon_user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_1 = skill[skill['skill_master_id'].notna()]\n",
    "unique_skill = skill_1[['anon_user_id', 'skill']].drop_duplicates()\n",
    "unique_skill = unique_skill.rename(columns={'skill': 'value'})\n",
    "\n",
    "# Assuming 'created_by' and 'achievement' columns exist in the 'achievement' DataFrame\n",
    "achievement_1 = achievement[achievement['experience_master_id'].notna()]\n",
    "unique_achievement = achievement_1[['anon_user_id', 'achievement']].drop_duplicates()\n",
    "unique_achievement = unique_achievement.rename(columns={'achievement': 'value'})\n",
    "\n",
    "# Concatenate the results\n",
    "concatenated_df = pd.concat([unique_skill, unique_achievement], ignore_index=True)\n",
    "\n",
    "#filtered_role = role_2[role_2['anon_user_id'].isin(final_id['anon_user_id'])]\n",
    "filtered_concatenated_df = concatenated_df[concatenated_df['anon_user_id'].isin(final_id['anon_user_id'])]\n",
    "\n",
    "skill_cate = pd.read_csv('skill_cate.csv')\n",
    "\n",
    "merged_df = pd.merge(filtered_concatenated_df, skill_cate, left_on='value', right_on='value', how='left')\n",
    "\n",
    "# Group by 'cate' and count unique 'created_by'\n",
    "result = merged_df.groupby('cate')['anon_user_id'].nunique().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "result = result.rename(columns={'anon_user_id': 'unique_people_count'})\n",
    "\n",
    "result.to_csv('cate_distribution.csv')\n",
    "\n",
    "merged_df = merged_df.merge(filtered_role[['anon_user_id', 'occupation_name', 'occupation_group_name']], \n",
    "                                  on='anon_user_id', how='left')\n",
    "\n",
    "merged_df = pd.merge(merged_df, final_id, on='anon_user_id', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_mapping = {\n",
    "    'Software QA Engineer / Tester': 'Testing & Validation',\n",
    "    'QA Manager': 'Testing & Validation',\n",
    "    'Validation Engineer': 'Testing & Validation',\n",
    "    'Embedded Systems Engineers': 'Embedded System',\n",
    "    'Software Architect': 'Architecture',\n",
    "    'Systems Architect': 'Architecture',\n",
    "    'Machine Learning Engineers': 'Machine Learning & Data',\n",
    "    'Data Scientist': 'Machine Learning & Data'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'occupation_name' column to create a new column 'job_cate'\n",
    "merged_df['job_cate'] = merged_df['occupation_name'].map(occupation_mapping)\n",
    "\n",
    "# If there are occupation names that don't match the mapping, you can handle them by filling with 'Unknown' or another category\n",
    "merged_df['job_cate'] = merged_df['job_cate'].fillna('General Software Dev')\n",
    "\n",
    "merged_df.loc[merged_df['cate'] == 'Embedded Systems', 'job_cate'] = 'Embedded System'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  job_cate  distinct_user_count  percentage\n",
      "0             Architecture                   12    0.023121\n",
      "1          Embedded System                  121    0.233141\n",
      "2     General Software Dev                  429    0.826590\n",
      "3  Machine Learning & Data                   34    0.065511\n",
      "4     Testing & Validation                   22    0.042389\n"
     ]
    }
   ],
   "source": [
    "distinct_user_count = merged_df.groupby('job_cate')['anon_user_id'].nunique().reset_index(name='distinct_user_count')\n",
    "\n",
    "# Step 2: Calculate the total number of distinct users across all job_cates\n",
    "total_users = filtered_role['anon_user_id'].nunique()\n",
    "\n",
    "# Step 3: Calculate the percentage of each job_cate\n",
    "distinct_user_count['percentage'] = (distinct_user_count['distinct_user_count'] / total_users)\n",
    "\n",
    "# Display the result\n",
    "print(distinct_user_count)\n",
    "\n",
    "distinct_user_count.to_csv('woven_job_cate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top skill for country/employ type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_to_country = {\n",
    "    'Nihonbashi (Head Office)': 'Japan',\n",
    "    'Palo Alto': 'USA',\n",
    "    'Ann Arbor': 'USA',\n",
    "    'London': 'United Kingdom',\n",
    "    'Sunnyvale': 'USA',\n",
    "    'Remote California': 'USA',\n",
    "    'Remote United Kingdom': 'United Kingdom',\n",
    "    'Mountain View': 'USA',\n",
    "    'Remote Massachusetts': 'USA',\n",
    "    'Higashifuji': 'Japan',\n",
    "    'Remote Japan': 'Japan',\n",
    "    'Seattle': 'USA',\n",
    "    'Remote New York': 'USA',\n",
    "    'Remote Oregon': 'USA',\n",
    "    'Remote Washington': 'USA',\n",
    "    'Brooklyn': 'USA',\n",
    "    'Remote Colorado': 'USA',\n",
    "    'Toyota Higashifuji': 'Japan',\n",
    "    'Remote Illinois': 'USA',\n",
    "    'Remote New Jersey': 'USA',\n",
    "    'Remote Utah': 'USA',\n",
    "    'Remote North Carolina': 'USA',\n",
    "    'Remote Maryland': 'USA',\n",
    "    'Remote Texas': 'USA',\n",
    "    'Remote Nevada': 'USA'\n",
    "}\n",
    "\n",
    "# Assuming your DataFrame is called df and has a 'location' column\n",
    "merged_df['country'] = merged_df['location'].map(location_to_country)\n",
    "\n",
    "skill_df = pd.merge(skill_1, final_id, on='anon_user_id', how='inner')\n",
    "skill_df['country'] = skill_df['location'].map(location_to_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "country skill cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country                                  cate  cate_user_count  \\\n",
      "0            Japan           Electronics & Power Systems               11   \n",
      "1            Japan                      Embedded Systems               76   \n",
      "2            Japan                 Machine Learning & AI               50   \n",
      "3            Japan  Manufacturing & Process Optimization               19   \n",
      "4            Japan            Robotics & Control Systems               10   \n",
      "5            Japan                 Simulation & Modeling               31   \n",
      "6              USA           Electronics & Power Systems               15   \n",
      "7              USA                      Embedded Systems               38   \n",
      "8              USA                   Infotainment System                1   \n",
      "9              USA                 Machine Learning & AI               53   \n",
      "10             USA  Manufacturing & Process Optimization               27   \n",
      "11             USA            Robotics & Control Systems               18   \n",
      "12             USA                 Simulation & Modeling               19   \n",
      "13  United Kingdom                      Embedded Systems                3   \n",
      "14  United Kingdom                   Infotainment System                1   \n",
      "15  United Kingdom                 Machine Learning & AI               12   \n",
      "16  United Kingdom  Manufacturing & Process Optimization                2   \n",
      "17  United Kingdom                 Simulation & Modeling                2   \n",
      "\n",
      "    total_user_count  percentage  \n",
      "0                301    0.036545  \n",
      "1                301    0.252492  \n",
      "2                301    0.166113  \n",
      "3                301    0.063123  \n",
      "4                301    0.033223  \n",
      "5                301    0.102990  \n",
      "6                194    0.077320  \n",
      "7                194    0.195876  \n",
      "8                194    0.005155  \n",
      "9                194    0.273196  \n",
      "10               194    0.139175  \n",
      "11               194    0.092784  \n",
      "12               194    0.097938  \n",
      "13                20    0.150000  \n",
      "14                20    0.050000  \n",
      "15                20    0.600000  \n",
      "16                20    0.100000  \n",
      "17                20    0.100000  \n"
     ]
    }
   ],
   "source": [
    "cate_user_counts = merged_df.groupby(['country', 'cate'])['anon_user_id'].nunique().reset_index(name='cate_user_count')\n",
    "\n",
    "# Calculate the total unique count of users under each job_cate\n",
    "total_user_counts = merged_df.groupby('country')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Merge the two dataframes to calculate the percentage\n",
    "merged_counts = pd.merge(cate_user_counts, total_user_counts, on='country')\n",
    "\n",
    "# Calculate the percentage of users in each cate under each job_cate\n",
    "merged_counts['percentage'] = merged_counts['cate_user_count'] / merged_counts['total_user_count']\n",
    "\n",
    "# Display the final result\n",
    "print(merged_counts[['country', 'cate', 'cate_user_count', 'total_user_count', 'percentage']])\n",
    "\n",
    "merged_counts.to_csv('woven_country_skill_cate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "employee type skill cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          employee_type                                  cate  \\\n",
      "0             Full Time           Electronics & Power Systems   \n",
      "1             Full Time                      Embedded Systems   \n",
      "2             Full Time                   Infotainment System   \n",
      "3             Full Time                 Machine Learning & AI   \n",
      "4             Full Time  Manufacturing & Process Optimization   \n",
      "5             Full Time            Robotics & Control Systems   \n",
      "6             Full Time                 Simulation & Modeling   \n",
      "7              Secondee           Electronics & Power Systems   \n",
      "8              Secondee                      Embedded Systems   \n",
      "9              Secondee                 Machine Learning & AI   \n",
      "10             Secondee  Manufacturing & Process Optimization   \n",
      "11             Secondee            Robotics & Control Systems   \n",
      "12             Secondee                 Simulation & Modeling   \n",
      "13  Secondee Concurrent                      Embedded Systems   \n",
      "14    Secondee Outbound                      Embedded Systems   \n",
      "15    Secondee Outbound                 Machine Learning & AI   \n",
      "16     Secondee Trainee  Manufacturing & Process Optimization   \n",
      "\n",
      "    cate_user_count  total_user_count  percentage  \n",
      "0                16               312    0.051282  \n",
      "1                69               312    0.221154  \n",
      "2                 2               312    0.006410  \n",
      "3                89               312    0.285256  \n",
      "4                33               312    0.105769  \n",
      "5                21               312    0.067308  \n",
      "6                26               312    0.083333  \n",
      "7                10               195    0.051282  \n",
      "8                46               195    0.235897  \n",
      "9                25               195    0.128205  \n",
      "10               14               195    0.071795  \n",
      "11                7               195    0.035897  \n",
      "12               26               195    0.133333  \n",
      "13                1                 2    0.500000  \n",
      "14                1                 1    1.000000  \n",
      "15                1                 1    1.000000  \n",
      "16                1                 2    0.500000  \n"
     ]
    }
   ],
   "source": [
    "cate_user_counts = merged_df.groupby(['employee_type', 'cate'])['anon_user_id'].nunique().reset_index(name='cate_user_count')\n",
    "\n",
    "# Calculate the total unique count of users under each job_cate\n",
    "total_user_counts = merged_df.groupby('employee_type')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Merge the two dataframes to calculate the percentage\n",
    "merged_counts = pd.merge(cate_user_counts, total_user_counts, on='employee_type')\n",
    "\n",
    "# Calculate the percentage of users in each cate under each job_cate\n",
    "merged_counts['percentage'] = merged_counts['cate_user_count'] / merged_counts['total_user_count']\n",
    "\n",
    "# Display the final result\n",
    "print(merged_counts[['employee_type', 'cate', 'cate_user_count', 'total_user_count', 'percentage']])\n",
    "\n",
    "merged_counts.to_csv('woven_employee_type_skill_cate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "country top skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            country                                              skill  \\\n",
      "43            Japan                       Product Lifecycle Management   \n",
      "72            Japan                                      Visual Design   \n",
      "4             Japan                         Artificial Neural Networks   \n",
      "10            Japan                          C# (Programming Language)   \n",
      "31            Japan                                 Interaction Design   \n",
      "66            Japan                                       Unity Engine   \n",
      "15            Japan                     Cross-Functional Collaboration   \n",
      "7             Japan                                   Business Process   \n",
      "30            Japan                                 Information Design   \n",
      "54            Japan                                           Simulink   \n",
      "74            Japan                                Workflow Management   \n",
      "61            Japan                     Systems Development Life Cycle   \n",
      "8             Japan                              Business Requirements   \n",
      "41            Japan                                Process Improvement   \n",
      "6             Japan                                      Business Case   \n",
      "29            Japan                                  Image Recognition   \n",
      "27            Japan                              Human-Centered Design   \n",
      "68            Japan                  User Experience (UX) Design Tools   \n",
      "5             Japan                                Autonomous Vehicles   \n",
      "26            Japan                                             GitHub   \n",
      "99              USA                                         Hand Tools   \n",
      "124             USA  SLAM Algorithms (Simultaneous Localization And...   \n",
      "95              USA                                  Electrical Wiring   \n",
      "127             USA                               Software Development   \n",
      "113             USA                                  Product Execution   \n",
      "97              USA                                                Git   \n",
      "89              USA                     Cross-Functional Collaboration   \n",
      "111             USA                               Power Tool Operation   \n",
      "91              USA                                          Debugging   \n",
      "81              USA                                      Business Case   \n",
      "87              USA                                    Computer Vision   \n",
      "137             USA                                       Unit Testing   \n",
      "90              USA                                 Data Visualization   \n",
      "85              USA                                                C++   \n",
      "103             USA                                              Linux   \n",
      "79              USA            Application Programming Interface (API)   \n",
      "120             USA                                             Python   \n",
      "80              USA                                Autonomous Vehicles   \n",
      "83              USA                              Business Requirements   \n",
      "92              USA                                      Deep Learning   \n",
      "144  United Kingdom                                          Debugging   \n",
      "148  United Kingdom                                             Python   \n",
      "145  United Kingdom                                                Git   \n",
      "143  United Kingdom                                    Computer Vision   \n",
      "150  United Kingdom                               Software Development   \n",
      "151  United Kingdom                                       Unit Testing   \n",
      "139  United Kingdom                                  Agile Methodology   \n",
      "141  United Kingdom                                                C++   \n",
      "140  United Kingdom            Application Programming Interface (API)   \n",
      "146  United Kingdom                                              Linux   \n",
      "147  United Kingdom                 PyTorch (Machine Learning Library)   \n",
      "142  United Kingdom                                              CI/CD   \n",
      "149  United Kingdom                                                SQL   \n",
      "\n",
      "     unique_user_count  total_user_count  percentage  average_skill_level  \n",
      "43                   7               301    0.023256             6.000000  \n",
      "72                   6               301    0.019934             5.666667  \n",
      "4                    9               301    0.029900             5.222222  \n",
      "10                   6               301    0.019934             5.166667  \n",
      "31                   7               301    0.023256             5.142857  \n",
      "66                   7               301    0.023256             5.142857  \n",
      "15                  38               301    0.126246             5.105263  \n",
      "7                   10               301    0.033223             5.100000  \n",
      "30                   6               301    0.019934             5.000000  \n",
      "54                   8               301    0.026578             5.000000  \n",
      "74                  18               301    0.059801             4.944444  \n",
      "61                   9               301    0.029900             4.888889  \n",
      "8                   13               301    0.043189             4.846154  \n",
      "41                  31               301    0.102990             4.806452  \n",
      "6                    9               301    0.029900             4.777778  \n",
      "29                  16               301    0.053156             4.750000  \n",
      "27                  11               301    0.036545             4.727273  \n",
      "68                  11               301    0.036545             4.727273  \n",
      "5                    7               301    0.023256             4.714286  \n",
      "26                  15               301    0.049834             4.666667  \n",
      "99                  11               193    0.056995             5.909091  \n",
      "124                  6               193    0.031088             5.666667  \n",
      "95                   8               193    0.041451             5.500000  \n",
      "127                 79               193    0.409326             5.392405  \n",
      "113                 18               193    0.093264             5.333333  \n",
      "97                  67               193    0.347150             5.298507  \n",
      "89                  35               193    0.181347             5.285714  \n",
      "111                  7               193    0.036269             5.285714  \n",
      "91                  60               193    0.310881             5.266667  \n",
      "81                   9               193    0.046632             5.222222  \n",
      "87                  11               193    0.056995             5.181818  \n",
      "137                 56               193    0.290155             5.178571  \n",
      "90                  12               193    0.062176             5.166667  \n",
      "85                  75               193    0.388601             5.160000  \n",
      "103                 60               193    0.310881             5.133333  \n",
      "79                  36               193    0.186528             5.111111  \n",
      "120                 90               193    0.466321             5.100000  \n",
      "80                   9               193    0.046632             5.000000  \n",
      "83                  12               193    0.062176             5.000000  \n",
      "92                   9               193    0.046632             5.000000  \n",
      "144                  9                20    0.450000             5.666667  \n",
      "148                 15                20    0.750000             5.600000  \n",
      "145                  9                20    0.450000             5.555556  \n",
      "143                  6                20    0.300000             5.500000  \n",
      "150                 14                20    0.700000             5.500000  \n",
      "151                  8                20    0.400000             5.500000  \n",
      "139                  9                20    0.450000             5.333333  \n",
      "141                 11                20    0.550000             5.181818  \n",
      "140                  6                20    0.300000             5.166667  \n",
      "146                  9                20    0.450000             5.111111  \n",
      "147                  6                20    0.300000             5.000000  \n",
      "142                  9                20    0.450000             4.666667  \n",
      "149                  9                20    0.450000             4.444444  \n"
     ]
    }
   ],
   "source": [
    "unique_counts = skill_df.groupby(['country', 'skill'])['anon_user_id'].nunique().reset_index(name='unique_user_count')\n",
    "\n",
    "# Step 2: Filter for skills with more than 5 unique anon_user_id\n",
    "filtered_skills = unique_counts[unique_counts['unique_user_count'] > 5]\n",
    "\n",
    "# Step 3: Calculate the average skill_level for these filtered skills\n",
    "average_skill_levels = skill_df.groupby(['country', 'skill'])['skill_level'].mean().reset_index(name='average_skill_level')\n",
    "\n",
    "# Step 4: Calculate total unique users per country\n",
    "total_unique_users_per_country = skill_df.groupby('country')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 5: Merge filtered skills with total user counts to calculate percentage\n",
    "filtered_skills_with_total = pd.merge(filtered_skills, total_unique_users_per_country, on='country')\n",
    "\n",
    "# Step 6: Calculate the percentage of unique users for each skill relative to the total count of the country\n",
    "filtered_skills_with_total['percentage'] = (filtered_skills_with_total['unique_user_count'] / filtered_skills_with_total['total_user_count'])\n",
    "\n",
    "# Step 7: Merge to get the average skill level for filtered skills\n",
    "merged = pd.merge(filtered_skills_with_total, average_skill_levels, on=['country', 'skill'])\n",
    "\n",
    "# Step 8: Sort and select the top 20 skills for each country based on average skill level\n",
    "top_skills = merged.sort_values(by=['country', 'average_skill_level'], ascending=[True, False])\n",
    "top_20_per_country = top_skills.groupby('country').head(20)\n",
    "\n",
    "# Display the result\n",
    "print(top_20_per_country)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_20_per_country.to_csv('top_20_per_country.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            country                                     skill  \\\n",
      "24            Japan                         Functional Safety   \n",
      "33            Japan                                   Jenkins   \n",
      "21            Japan                                    Docker   \n",
      "16            Japan                        Data Visualization   \n",
      "28            Japan                                 ISO 26262   \n",
      "2             Japan                 Amazon Web Services (AWS)   \n",
      "36            Japan                  Manufacturing Automation   \n",
      "23            Japan                   Figma (Design Software)   \n",
      "50            Japan                        Quality Management   \n",
      "52            Japan             Scikit-Learn (Python Package)   \n",
      "55            Japan                           Software Design   \n",
      "14            Japan                     Computer-Aided Design   \n",
      "39            Japan                       Performance Testing   \n",
      "12            Japan                                     CI/CD   \n",
      "37            Japan                    NumPy (Python Package)   \n",
      "51            Japan                                       SQL   \n",
      "38            Japan                   Pandas (Python Package)   \n",
      "40            Japan                                PostgreSQL   \n",
      "3             Japan   Application Programming Interface (API)   \n",
      "22            Japan                         Embedded Software   \n",
      "136             USA                                   Tooling   \n",
      "133             USA                           Test Automation   \n",
      "121             USA                        Quality Management   \n",
      "93              USA  Design Failure Mode And Effects Analysis   \n",
      "100             USA               Java (Programming Language)   \n",
      "102             USA                                   Jenkins   \n",
      "101             USA         JavaScript (Programming Language)   \n",
      "78              USA                 Amazon Web Services (AWS)   \n",
      "104             USA                                    MATLAB   \n",
      "82              USA                          Business Process   \n",
      "114             USA              Product Lifecycle Management   \n",
      "116             USA                          Product Roadmaps   \n",
      "125             USA                                       SQL   \n",
      "117             USA                          Product Strategy   \n",
      "94              USA                                    Docker   \n",
      "134             USA                          Test Engineering   \n",
      "122             USA                        Regression Testing   \n",
      "112             USA                       Process Improvement   \n",
      "131             USA                            System Testing   \n",
      "98              USA                     Go-to-Market Strategy   \n",
      "149  United Kingdom                                       SQL   \n",
      "142  United Kingdom                                     CI/CD   \n",
      "147  United Kingdom        PyTorch (Machine Learning Library)   \n",
      "146  United Kingdom                                     Linux   \n",
      "140  United Kingdom   Application Programming Interface (API)   \n",
      "141  United Kingdom                                       C++   \n",
      "139  United Kingdom                         Agile Methodology   \n",
      "143  United Kingdom                           Computer Vision   \n",
      "150  United Kingdom                      Software Development   \n",
      "151  United Kingdom                              Unit Testing   \n",
      "145  United Kingdom                                       Git   \n",
      "148  United Kingdom                                    Python   \n",
      "144  United Kingdom                                 Debugging   \n",
      "\n",
      "     unique_user_count  total_user_count  percentage  average_skill_level  \n",
      "24                   9               301    0.029900             3.111111  \n",
      "33                  37               301    0.122924             3.162162  \n",
      "21                  25               301    0.083056             3.280000  \n",
      "16                   8               301    0.026578             3.375000  \n",
      "28                   8               301    0.026578             3.375000  \n",
      "2                   33               301    0.109635             3.393939  \n",
      "36                   7               301    0.023256             3.428571  \n",
      "23                  12               301    0.039867             3.500000  \n",
      "50                   6               301    0.019934             3.500000  \n",
      "52                   6               301    0.019934             3.500000  \n",
      "55                  12               301    0.039867             3.500000  \n",
      "14                   7               301    0.023256             3.571429  \n",
      "39                   7               301    0.023256             3.571429  \n",
      "12                  68               301    0.225914             3.617647  \n",
      "37                   9               301    0.029900             3.666667  \n",
      "51                  28               301    0.093023             3.785714  \n",
      "38                  14               301    0.046512             3.857143  \n",
      "40                   7               301    0.023256             3.857143  \n",
      "3                   30               301    0.099668             3.866667  \n",
      "22                  46               301    0.152824             3.934783  \n",
      "136                  7               193    0.036269             3.571429  \n",
      "133                 12               193    0.062176             3.833333  \n",
      "121                  7               193    0.036269             3.857143  \n",
      "93                   6               193    0.031088             4.000000  \n",
      "100                  7               193    0.036269             4.000000  \n",
      "102                 19               193    0.098446             4.000000  \n",
      "101                 10               193    0.051813             4.100000  \n",
      "78                  13               193    0.067358             4.153846  \n",
      "104                  6               193    0.031088             4.166667  \n",
      "82                  14               193    0.072539             4.285714  \n",
      "114                  6               193    0.031088             4.333333  \n",
      "116                 24               193    0.124352             4.333333  \n",
      "125                 35               193    0.181347             4.342857  \n",
      "117                 20               193    0.103627             4.350000  \n",
      "94                   8               193    0.041451             4.375000  \n",
      "134                 17               193    0.088083             4.411765  \n",
      "122                  9               193    0.046632             4.444444  \n",
      "112                 36               193    0.186528             4.472222  \n",
      "131                  8               193    0.041451             4.500000  \n",
      "98                   7               193    0.036269             4.571429  \n",
      "149                  9                20    0.450000             4.444444  \n",
      "142                  9                20    0.450000             4.666667  \n",
      "147                  6                20    0.300000             5.000000  \n",
      "146                  9                20    0.450000             5.111111  \n",
      "140                  6                20    0.300000             5.166667  \n",
      "141                 11                20    0.550000             5.181818  \n",
      "139                  9                20    0.450000             5.333333  \n",
      "143                  6                20    0.300000             5.500000  \n",
      "150                 14                20    0.700000             5.500000  \n",
      "151                  8                20    0.400000             5.500000  \n",
      "145                  9                20    0.450000             5.555556  \n",
      "148                 15                20    0.750000             5.600000  \n",
      "144                  9                20    0.450000             5.666667  \n"
     ]
    }
   ],
   "source": [
    "unique_counts = skill_df.groupby(['country', 'skill'])['anon_user_id'].nunique().reset_index(name='unique_user_count')\n",
    "\n",
    "# Step 2: Filter for skills with more than 5 unique anon_user_id\n",
    "filtered_skills = unique_counts[unique_counts['unique_user_count'] > 5]\n",
    "\n",
    "# Step 3: Calculate the average skill_level for these filtered skills\n",
    "average_skill_levels = skill_df.groupby(['country', 'skill'])['skill_level'].mean().reset_index(name='average_skill_level')\n",
    "\n",
    "# Step 4: Calculate total unique users per country\n",
    "total_unique_users_per_country = skill_df.groupby('country')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 5: Merge filtered skills with total user counts to calculate percentage\n",
    "filtered_skills_with_total = pd.merge(filtered_skills, total_unique_users_per_country, on='country')\n",
    "\n",
    "# Step 6: Calculate the percentage of unique users for each skill relative to the total count of the country\n",
    "filtered_skills_with_total['percentage'] = (filtered_skills_with_total['unique_user_count'] / filtered_skills_with_total['total_user_count'])\n",
    "\n",
    "# Step 7: Merge to get the average skill level for filtered skills\n",
    "merged = pd.merge(filtered_skills_with_total, average_skill_levels, on=['country', 'skill'])\n",
    "\n",
    "# Step 8: Sort and select the top 20 skills for each country based on average skill level\n",
    "top_skills = merged.sort_values(by=['country', 'average_skill_level'], ascending=[True,True])\n",
    "top_20_per_country = top_skills.groupby('country').head(20)\n",
    "\n",
    "# Display the result\n",
    "print(top_20_per_country)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_20_per_country.to_csv('bottom_20_per_country.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country                                    skill  \\\n",
      "10           Japan           Cross-Functional Collaboration   \n",
      "4            Japan                         Business Process   \n",
      "50           Japan                      Workflow Management   \n",
      "5            Japan                    Business Requirements   \n",
      "26           Japan                      Process Improvement   \n",
      "20           Japan                        Image Recognition   \n",
      "19           Japan                    Human-Centered Design   \n",
      "45           Japan        User Experience (UX) Design Tools   \n",
      "18           Japan                                   GitHub   \n",
      "13           Japan                       Design Prototyping   \n",
      "38           Japan          Software Development Life Cycle   \n",
      "49           Japan                              Wireframing   \n",
      "39           Japan                          Sprint Planning   \n",
      "24           Japan                                   MATLAB   \n",
      "12           Japan                            Deep Learning   \n",
      "42           Japan                            Test Planning   \n",
      "37           Japan                     Software Development   \n",
      "27           Japan                        Product Execution   \n",
      "30           Japan                         Product Strategy   \n",
      "44           Japan                        Usability Testing   \n",
      "67             USA                               Hand Tools   \n",
      "85             USA                     Software Development   \n",
      "75             USA                        Product Execution   \n",
      "66             USA                                      Git   \n",
      "62             USA           Cross-Functional Collaboration   \n",
      "64             USA                                Debugging   \n",
      "60             USA                          Computer Vision   \n",
      "92             USA                             Unit Testing   \n",
      "63             USA                       Data Visualization   \n",
      "58             USA                                      C++   \n",
      "70             USA                                    Linux   \n",
      "54             USA  Application Programming Interface (API)   \n",
      "81             USA                                   Python   \n",
      "56             USA                    Business Requirements   \n",
      "82             USA                      Root Cause Analysis   \n",
      "93             USA                      Workflow Management   \n",
      "52             USA                        Agile Methodology   \n",
      "71             USA                        Machine Operation   \n",
      "73             USA                      Performance Testing   \n",
      "57             USA                 C (Programming Language)   \n",
      "95  United Kingdom                                   Python   \n",
      "96  United Kingdom                     Software Development   \n",
      "94  United Kingdom                                      C++   \n",
      "\n",
      "    unique_user_count  total_user_count  percentage  average_skill_level  \n",
      "10                 38               301    0.126246             5.105263  \n",
      "4                  10               301    0.033223             5.100000  \n",
      "50                 18               301    0.059801             4.944444  \n",
      "5                  13               301    0.043189             4.846154  \n",
      "26                 31               301    0.102990             4.806452  \n",
      "20                 16               301    0.053156             4.750000  \n",
      "19                 11               301    0.036545             4.727273  \n",
      "45                 11               301    0.036545             4.727273  \n",
      "18                 15               301    0.049834             4.666667  \n",
      "13                 11               301    0.036545             4.636364  \n",
      "38                 22               301    0.073090             4.636364  \n",
      "49                 10               301    0.033223             4.600000  \n",
      "39                 22               301    0.073090             4.590909  \n",
      "24                 19               301    0.063123             4.578947  \n",
      "12                 20               301    0.066445             4.550000  \n",
      "42                 20               301    0.066445             4.550000  \n",
      "37                111               301    0.368771             4.549550  \n",
      "27                 33               301    0.109635             4.545455  \n",
      "30                 33               301    0.109635             4.545455  \n",
      "44                 11               301    0.036545             4.545455  \n",
      "67                 11               193    0.056995             5.909091  \n",
      "85                 79               193    0.409326             5.392405  \n",
      "75                 18               193    0.093264             5.333333  \n",
      "66                 67               193    0.347150             5.298507  \n",
      "62                 35               193    0.181347             5.285714  \n",
      "64                 60               193    0.310881             5.266667  \n",
      "60                 11               193    0.056995             5.181818  \n",
      "92                 56               193    0.290155             5.178571  \n",
      "63                 12               193    0.062176             5.166667  \n",
      "58                 75               193    0.388601             5.160000  \n",
      "70                 60               193    0.310881             5.133333  \n",
      "54                 36               193    0.186528             5.111111  \n",
      "81                 90               193    0.466321             5.100000  \n",
      "56                 12               193    0.062176             5.000000  \n",
      "82                 14               193    0.072539             5.000000  \n",
      "93                 16               193    0.082902             5.000000  \n",
      "52                 61               193    0.316062             4.967213  \n",
      "71                 10               193    0.051813             4.900000  \n",
      "73                 10               193    0.051813             4.900000  \n",
      "57                 13               193    0.067358             4.846154  \n",
      "95                 15                20    0.750000             5.600000  \n",
      "96                 14                20    0.700000             5.500000  \n",
      "94                 11                20    0.550000             5.181818  \n"
     ]
    }
   ],
   "source": [
    "unique_counts = skill_df.groupby(['country', 'skill'])['anon_user_id'].nunique().reset_index(name='unique_user_count')\n",
    "filtered_skills = unique_counts[unique_counts['unique_user_count'] >= 10]\n",
    "\n",
    "# Step 3: Calculate the average skill_level for these filtered skills\n",
    "average_skill_levels = skill_df.groupby(['country', 'skill'])['skill_level'].mean().reset_index(name='average_skill_level')\n",
    "\n",
    "# Step 4: Calculate total unique users per country\n",
    "total_unique_users_per_country = skill_df.groupby('country')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 5: Merge filtered skills with total user counts to calculate percentage\n",
    "filtered_skills_with_total = pd.merge(filtered_skills, total_unique_users_per_country, on='country')\n",
    "\n",
    "# Step 6: Calculate the percentage of unique users for each skill relative to the total count of the country\n",
    "filtered_skills_with_total['percentage'] = (filtered_skills_with_total['unique_user_count'] / filtered_skills_with_total['total_user_count'])\n",
    "\n",
    "# Step 7: Merge to get the average skill level for filtered skills\n",
    "merged = pd.merge(filtered_skills_with_total, average_skill_levels, on=['country', 'skill'])\n",
    "\n",
    "# Step 8: Sort and select the top 20 skills for each country based on average skill level\n",
    "top_skills = merged.sort_values(by=['country', 'average_skill_level'], ascending=[True, False])\n",
    "top_20_per_country = top_skills.groupby('country').head(20)\n",
    "\n",
    "# Display the result\n",
    "print(top_20_per_country)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_20_per_country.to_csv('top_20_per_country_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country                                    skill  unique_user_count  \\\n",
      "7    Japan           Cross-Functional Collaboration                 38   \n",
      "15   Japan                      Process Improvement                 31   \n",
      "26   Japan          Software Development Life Cycle                 22   \n",
      "27   Japan                          Sprint Planning                 22   \n",
      "9    Japan                            Deep Learning                 20   \n",
      "28   Japan                            Test Planning                 20   \n",
      "25   Japan                     Software Development                111   \n",
      "16   Japan                        Product Execution                 33   \n",
      "19   Japan                         Product Strategy                 33   \n",
      "21   Japan       PyTorch (Machine Learning Library)                 26   \n",
      "18   Japan                         Product Roadmaps                 31   \n",
      "8    Japan                                Debugging                 67   \n",
      "20   Japan                         Project Planning                 55   \n",
      "12   Japan                                      Git                 91   \n",
      "24   Japan                                    Scrum                 38   \n",
      "1    Japan                        Agile Methodology                 82   \n",
      "17   Japan                     Product Requirements                 33   \n",
      "0    Japan       Advanced Driver Assistance Systems                 66   \n",
      "22   Japan                                   Python                128   \n",
      "29   Japan                             Unit Testing                 73   \n",
      "46     USA                     Software Development                 79   \n",
      "37     USA                                      Git                 67   \n",
      "35     USA           Cross-Functional Collaboration                 35   \n",
      "36     USA                                Debugging                 60   \n",
      "47     USA                             Unit Testing                 56   \n",
      "33     USA                                      C++                 75   \n",
      "38     USA                                    Linux                 60   \n",
      "32     USA  Application Programming Interface (API)                 36   \n",
      "44     USA                                   Python                 90   \n",
      "31     USA                        Agile Methodology                 61   \n",
      "30     USA       Advanced Driver Assistance Systems                 31   \n",
      "43     USA                         Project Planning                 35   \n",
      "34     USA                                    CI/CD                 48   \n",
      "40     USA                     Product Requirements                 23   \n",
      "39     USA                      Process Improvement                 36   \n",
      "42     USA                         Product Strategy                 20   \n",
      "45     USA                                      SQL                 35   \n",
      "41     USA                         Product Roadmaps                 24   \n",
      "\n",
      "    total_user_count  percentage  average_skill_level  \n",
      "7                301    0.126246             5.105263  \n",
      "15               301    0.102990             4.806452  \n",
      "26               301    0.073090             4.636364  \n",
      "27               301    0.073090             4.590909  \n",
      "9                301    0.066445             4.550000  \n",
      "28               301    0.066445             4.550000  \n",
      "25               301    0.368771             4.549550  \n",
      "16               301    0.109635             4.545455  \n",
      "19               301    0.109635             4.545455  \n",
      "21               301    0.086379             4.500000  \n",
      "18               301    0.102990             4.419355  \n",
      "8                301    0.222591             4.402985  \n",
      "20               301    0.182724             4.400000  \n",
      "12               301    0.302326             4.307692  \n",
      "24               301    0.126246             4.289474  \n",
      "1                301    0.272425             4.280488  \n",
      "17               301    0.109635             4.242424  \n",
      "0                301    0.219269             4.227273  \n",
      "22               301    0.425249             4.203125  \n",
      "29               301    0.242525             4.191781  \n",
      "46               193    0.409326             5.392405  \n",
      "37               193    0.347150             5.298507  \n",
      "35               193    0.181347             5.285714  \n",
      "36               193    0.310881             5.266667  \n",
      "47               193    0.290155             5.178571  \n",
      "33               193    0.388601             5.160000  \n",
      "38               193    0.310881             5.133333  \n",
      "32               193    0.186528             5.111111  \n",
      "44               193    0.466321             5.100000  \n",
      "31               193    0.316062             4.967213  \n",
      "30               193    0.160622             4.741935  \n",
      "43               193    0.181347             4.714286  \n",
      "34               193    0.248705             4.708333  \n",
      "40               193    0.119171             4.608696  \n",
      "39               193    0.186528             4.472222  \n",
      "42               193    0.103627             4.350000  \n",
      "45               193    0.181347             4.342857  \n",
      "41               193    0.124352             4.333333  \n"
     ]
    }
   ],
   "source": [
    "unique_counts = skill_df.groupby(['country', 'skill'])['anon_user_id'].nunique().reset_index(name='unique_user_count')\n",
    "filtered_skills = unique_counts[unique_counts['unique_user_count'] >= 20]\n",
    "\n",
    "# Step 3: Calculate the average skill_level for these filtered skills\n",
    "average_skill_levels = skill_df.groupby(['country', 'skill'])['skill_level'].mean().reset_index(name='average_skill_level')\n",
    "\n",
    "# Step 4: Calculate total unique users per country\n",
    "total_unique_users_per_country = skill_df.groupby('country')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 5: Merge filtered skills with total user counts to calculate percentage\n",
    "filtered_skills_with_total = pd.merge(filtered_skills, total_unique_users_per_country, on='country')\n",
    "\n",
    "# Step 6: Calculate the percentage of unique users for each skill relative to the total count of the country\n",
    "filtered_skills_with_total['percentage'] = (filtered_skills_with_total['unique_user_count'] / filtered_skills_with_total['total_user_count'])\n",
    "\n",
    "# Step 7: Merge to get the average skill level for filtered skills\n",
    "merged = pd.merge(filtered_skills_with_total, average_skill_levels, on=['country', 'skill'])\n",
    "\n",
    "# Step 8: Sort and select the top 20 skills for each country based on average skill level\n",
    "top_skills = merged.sort_values(by=['country', 'average_skill_level'], ascending=[True, False])\n",
    "top_20_per_country = top_skills.groupby('country').head(20)\n",
    "\n",
    "# Display the result\n",
    "print(top_20_per_country)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_20_per_country.to_csv('top_20_per_country_20.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "country+common skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             country                                        value  count  \\\n",
      "333            Japan                                       Python    128   \n",
      "393            Japan                         Software Development    111   \n",
      "23             Japan                   Agile Software Development    108   \n",
      "55             Japan                                          C++     92   \n",
      "169            Japan                                          Git     91   \n",
      "...              ...                                          ...    ...   \n",
      "1010  United Kingdom                         Database Development      4   \n",
      "1072  United Kingdom                            Product Execution      4   \n",
      "1078  United Kingdom                             Project Planning      4   \n",
      "1086  United Kingdom                          Root Cause Analysis      4   \n",
      "1101  United Kingdom  Software Testing And Quality Assurance (QA)      4   \n",
      "\n",
      "      total_user_count  percentage  \n",
      "333                301    0.425249  \n",
      "393                301    0.368771  \n",
      "23                 301    0.358804  \n",
      "55                 301    0.305648  \n",
      "169                301    0.302326  \n",
      "...                ...         ...  \n",
      "1010                20    0.200000  \n",
      "1072                20    0.200000  \n",
      "1078                20    0.200000  \n",
      "1086                20    0.200000  \n",
      "1101                20    0.200000  \n",
      "\n",
      "[90 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "item_counts = merged_df.groupby(['country', 'value'])['anon_user_id'].nunique().reset_index(name='count')\n",
    "\n",
    "# Step 2: Calculate the total count of 'anon_user_id' for each 'job_cate'\n",
    "total_user_counts =  merged_df.groupby('country')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 3: Calculate the percentage for each 'value' under each 'job_cate'\n",
    "\n",
    "merged_counts = pd.merge(item_counts, total_user_counts, on='country')\n",
    "\n",
    "merged_counts['percentage'] = merged_counts['count'] / merged_counts['total_user_count']\n",
    "\n",
    "# Step 4: Sort by 'job_cate' and 'count' (descending), and keep the top N items for each 'job_cate'\n",
    "most_frequent_items = merged_counts.sort_values(['country', 'count'], ascending=[True, False])\n",
    "\n",
    "# Step 5: Select the top N items for each 'job_cate' (N=20 in this case)\n",
    "top_n_items_per_cate = most_frequent_items.groupby('country').head(30)\n",
    "\n",
    "# Output the result\n",
    "print(top_n_items_per_cate)\n",
    "\n",
    "top_n_items_per_cate.to_csv('woven_country_commonskill.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "employee type top skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    employee_type                                              skill  \\\n",
      "30      Full Time                                         Hand Tools   \n",
      "61      Full Time  SLAM Algorithms (Simultaneous Localization And...   \n",
      "4       Full Time                         Artificial Neural Networks   \n",
      "24      Full Time                                  Electrical Wiring   \n",
      "64      Full Time                                    Software Design   \n",
      "22      Full Time                                 Design Prototyping   \n",
      "16      Full Time                     Cross-Functional Collaboration   \n",
      "33      Full Time                                 Interaction Design   \n",
      "15      Full Time                                     Control Theory   \n",
      "31      Full Time                              Human-Centered Design   \n",
      "70      Full Time                               Systems Architecture   \n",
      "47      Full Time                               Power Tool Operation   \n",
      "32      Full Time                                  Image Recognition   \n",
      "65      Full Time                               Software Development   \n",
      "19      Full Time                                          Debugging   \n",
      "12      Full Time                                    Computer Vision   \n",
      "5       Full Time                                Autonomous Vehicles   \n",
      "27      Full Time                                                Git   \n",
      "43      Full Time                                    Motion Planning   \n",
      "56      Full Time                                             Python   \n",
      "124      Secondee                                           Simulink   \n",
      "133      Secondee                                Workflow Management   \n",
      "97       Secondee                     Cross-Functional Collaboration   \n",
      "100      Secondee                                      Deep Learning   \n",
      "107      Secondee                                  Image Recognition   \n",
      "117      Secondee                                   Product Strategy   \n",
      "110      Secondee                                             MATLAB   \n",
      "116      Secondee                                   Product Roadmaps   \n",
      "128      Secondee                                    Sprint Planning   \n",
      "114      Secondee                                  Product Execution   \n",
      "101      Secondee           Design Failure Mode And Effects Analysis   \n",
      "119      Secondee                 PyTorch (Machine Learning Library)   \n",
      "113      Secondee                                Process Improvement   \n",
      "131      Secondee                                      Test Planning   \n",
      "127      Secondee                    Software Development Life Cycle   \n",
      "88       Secondee                 Advanced Driver Assistance Systems   \n",
      "95       Secondee                                    Computer Vision   \n",
      "118      Secondee                                   Project Planning   \n",
      "130      Secondee                                   Test Engineering   \n",
      "126      Secondee                               Software Development   \n",
      "\n",
      "     unique_user_count  total_user_count  percentage  average_skill_level  \n",
      "30                  11               311    0.035370             5.909091  \n",
      "61                  11               311    0.035370             5.818182  \n",
      "4                   13               311    0.041801             5.461538  \n",
      "24                   9               311    0.028939             5.444444  \n",
      "64                   7               311    0.022508             5.428571  \n",
      "22                  10               311    0.032154             5.400000  \n",
      "16                  51               311    0.163987             5.392157  \n",
      "33                   8               311    0.025723             5.375000  \n",
      "15                   6               311    0.019293             5.333333  \n",
      "31                   9               311    0.028939             5.333333  \n",
      "70                   6               311    0.019293             5.333333  \n",
      "47                   7               311    0.022508             5.285714  \n",
      "32                  11               311    0.035370             5.272727  \n",
      "65                 141               311    0.453376             5.269504  \n",
      "19                  95               311    0.305466             5.242105  \n",
      "12                  25               311    0.080386             5.240000  \n",
      "5                   12               311    0.038585             5.166667  \n",
      "27                 112               311    0.360129             5.133929  \n",
      "43                   8               311    0.025723             5.125000  \n",
      "56                 150               311    0.482315             5.106667  \n",
      "124                  8               195    0.041026             5.000000  \n",
      "133                 10               195    0.051282             5.000000  \n",
      "97                  25               195    0.128205             4.880000  \n",
      "100                 10               195    0.051282             4.800000  \n",
      "107                 10               195    0.051282             4.700000  \n",
      "117                 21               195    0.107692             4.619048  \n",
      "110                 18               195    0.092308             4.611111  \n",
      "116                 18               195    0.092308             4.555556  \n",
      "128                 11               195    0.056410             4.545455  \n",
      "114                 21               195    0.107692             4.523810  \n",
      "101                  8               195    0.041026             4.500000  \n",
      "119                 13               195    0.066667             4.384615  \n",
      "113                 16               195    0.082051             4.375000  \n",
      "131                 14               195    0.071795             4.357143  \n",
      "127                 12               195    0.061538             4.333333  \n",
      "88                  49               195    0.251282             4.326531  \n",
      "95                  10               195    0.051282             4.300000  \n",
      "118                 37               195    0.189744             4.297297  \n",
      "130                 12               195    0.061538             4.250000  \n",
      "126                 62               195    0.317949             4.225806  \n"
     ]
    }
   ],
   "source": [
    "unique_counts = skill_df.groupby(['employee_type', 'skill'])['anon_user_id'].nunique().reset_index(name='unique_user_count')\n",
    "\n",
    "# Step 2: Filter for skills with more than 5 unique anon_user_id\n",
    "filtered_skills = unique_counts[unique_counts['unique_user_count'] > 5]\n",
    "\n",
    "# Step 3: Calculate the average skill_level for these filtered skills\n",
    "average_skill_levels = skill_df.groupby(['employee_type', 'skill'])['skill_level'].mean().reset_index(name='average_skill_level')\n",
    "\n",
    "# Step 4: Calculate total unique users per country\n",
    "total_unique_users_per_country = skill_df.groupby('employee_type')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 5: Merge filtered skills with total user counts to calculate percentage\n",
    "filtered_skills_with_total = pd.merge(filtered_skills, total_unique_users_per_country, on='employee_type')\n",
    "\n",
    "# Step 6: Calculate the percentage of unique users for each skill relative to the total count of the country\n",
    "filtered_skills_with_total['percentage'] = (filtered_skills_with_total['unique_user_count'] / filtered_skills_with_total['total_user_count'])\n",
    "\n",
    "# Step 7: Merge to get the average skill level for filtered skills\n",
    "merged = pd.merge(filtered_skills_with_total, average_skill_levels, on=['employee_type', 'skill'])\n",
    "\n",
    "# Step 8: Sort and select the top 20 skills for each country based on average skill level\n",
    "top_skills = merged.sort_values(by=['employee_type', 'average_skill_level'], ascending=[True, False])\n",
    "top_20_per_country = top_skills.groupby('employee_type').head(20)\n",
    "\n",
    "# Display the result\n",
    "print(top_20_per_country)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_20_per_country.to_csv('top_20_per_employeetype.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "employee type+bottom skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    employee_type                                     skill  \\\n",
      "41      Full Time                  Manufacturing Automation   \n",
      "36      Full Time                                   Jenkins   \n",
      "77      Full Time                                   Tooling   \n",
      "46      Full Time                                PostgreSQL   \n",
      "60      Full Time               Rust (Programming Language)   \n",
      "21      Full Time  Design Failure Mode And Effects Analysis   \n",
      "74      Full Time                           Test Automation   \n",
      "38      Full Time                                    MATLAB   \n",
      "87      Full Time                     Workshop Facilitation   \n",
      "2       Full Time                 Amazon Web Services (AWS)   \n",
      "57      Full Time                        Quality Management   \n",
      "23      Full Time                                    Docker   \n",
      "34      Full Time               Java (Programming Language)   \n",
      "35      Full Time         JavaScript (Programming Language)   \n",
      "73      Full Time                                 Terraform   \n",
      "7       Full Time                          Business Process   \n",
      "72      Full Time                                TensorFlow   \n",
      "52      Full Time                          Product Roadmaps   \n",
      "62      Full Time                                       SQL   \n",
      "25      Full Time                         Embedded Software   \n",
      "102      Secondee                                    Docker   \n",
      "104      Secondee                         Functional Safety   \n",
      "106      Secondee                                 ISO 26262   \n",
      "90       Secondee                 Amazon Web Services (AWS)   \n",
      "94       Secondee                                     CI/CD   \n",
      "121      Secondee                                       SQL   \n",
      "108      Secondee                                   Jenkins   \n",
      "91       Secondee   Application Programming Interface (API)   \n",
      "111      Secondee                    NumPy (Python Package)   \n",
      "122      Secondee             Scikit-Learn (Python Package)   \n",
      "125      Secondee                           Software Design   \n",
      "96       Secondee                     Computer-Aided Design   \n",
      "112      Secondee                   Pandas (Python Package)   \n",
      "129      Secondee                           Test Automation   \n",
      "132      Secondee                              Unit Testing   \n",
      "120      Secondee                                    Python   \n",
      "93       Secondee                                       C++   \n",
      "98       Secondee                        Data Visualization   \n",
      "109      Secondee                                     Linux   \n",
      "103      Secondee                         Embedded Software   \n",
      "\n",
      "     unique_user_count  total_user_count  percentage  average_skill_level  \n",
      "41                   9               311    0.028939             2.777778  \n",
      "36                  38               311    0.122186             3.552632  \n",
      "77                   7               311    0.022508             3.571429  \n",
      "46                  11               311    0.035370             3.909091  \n",
      "60                  11               311    0.035370             3.909091  \n",
      "21                   6               311    0.019293             4.000000  \n",
      "74                  19               311    0.061093             4.052632  \n",
      "38                   7               311    0.022508             4.142857  \n",
      "87                   6               311    0.019293             4.166667  \n",
      "2                   25               311    0.080386             4.200000  \n",
      "57                  10               311    0.032154             4.200000  \n",
      "23                  22               311    0.070740             4.227273  \n",
      "34                  15               311    0.048232             4.266667  \n",
      "35                  22               311    0.070740             4.272727  \n",
      "73                   7               311    0.022508             4.285714  \n",
      "7                   20               311    0.064309             4.300000  \n",
      "72                   6               311    0.019293             4.333333  \n",
      "52                  39               311    0.125402             4.358974  \n",
      "62                  57               311    0.183280             4.368421  \n",
      "25                  33               311    0.106109             4.393939  \n",
      "102                 10               195    0.051282             2.300000  \n",
      "104                  7               195    0.035897             2.857143  \n",
      "106                  6               195    0.030769             3.000000  \n",
      "90                  23               195    0.117949             3.130435  \n",
      "94                  32               195    0.164103             3.156250  \n",
      "121                 14               195    0.071795             3.214286  \n",
      "108                 20               195    0.102564             3.300000  \n",
      "91                  16               195    0.082051             3.375000  \n",
      "111                  7               195    0.035897             3.428571  \n",
      "122                  6               195    0.030769             3.500000  \n",
      "125                  8               195    0.041026             3.500000  \n",
      "96                   7               195    0.035897             3.571429  \n",
      "112                 10               195    0.051282             3.600000  \n",
      "129                  9               195    0.046154             3.777778  \n",
      "132                 40               195    0.205128             3.800000  \n",
      "120                 80               195    0.410256             3.812500  \n",
      "93                  50               195    0.256410             3.820000  \n",
      "98                   6               195    0.030769             3.833333  \n",
      "109                 46               195    0.235897             3.847826  \n",
      "103                 32               195    0.164103             3.968750  \n"
     ]
    }
   ],
   "source": [
    "unique_counts = skill_df.groupby(['employee_type', 'skill'])['anon_user_id'].nunique().reset_index(name='unique_user_count')\n",
    "\n",
    "# Step 2: Filter for skills with more than 5 unique anon_user_id\n",
    "filtered_skills = unique_counts[unique_counts['unique_user_count'] > 5]\n",
    "\n",
    "# Step 3: Calculate the average skill_level for these filtered skills\n",
    "average_skill_levels = skill_df.groupby(['employee_type', 'skill'])['skill_level'].mean().reset_index(name='average_skill_level')\n",
    "\n",
    "# Step 4: Calculate total unique users per country\n",
    "total_unique_users_per_country = skill_df.groupby('employee_type')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 5: Merge filtered skills with total user counts to calculate percentage\n",
    "filtered_skills_with_total = pd.merge(filtered_skills, total_unique_users_per_country, on='employee_type')\n",
    "\n",
    "# Step 6: Calculate the percentage of unique users for each skill relative to the total count of the country\n",
    "filtered_skills_with_total['percentage'] = (filtered_skills_with_total['unique_user_count'] / filtered_skills_with_total['total_user_count'])\n",
    "\n",
    "# Step 7: Merge to get the average skill level for filtered skills\n",
    "merged = pd.merge(filtered_skills_with_total, average_skill_levels, on=['employee_type', 'skill'])\n",
    "\n",
    "# Step 8: Sort and select the top 20 skills for each country based on average skill level\n",
    "top_skills = merged.sort_values(by=['employee_type', 'average_skill_level'], ascending=[True,True])\n",
    "top_20_per_country = top_skills.groupby('employee_type').head(20)\n",
    "\n",
    "# Display the result\n",
    "print(top_20_per_country)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "top_20_per_country.to_csv('bottom_20_per_employeetype.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "woven job cate skill cate cross analysis - don't review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   job_cate                                  cate  \\\n",
      "0              Architecture           Electronics & Power Systems   \n",
      "1              Architecture                 Machine Learning & AI   \n",
      "2              Architecture  Manufacturing & Process Optimization   \n",
      "3              Architecture                 Simulation & Modeling   \n",
      "4           Embedded System           Electronics & Power Systems   \n",
      "5           Embedded System                      Embedded Systems   \n",
      "6           Embedded System                   Infotainment System   \n",
      "7           Embedded System                 Machine Learning & AI   \n",
      "8           Embedded System            Robotics & Control Systems   \n",
      "9           Embedded System                 Simulation & Modeling   \n",
      "10     General Software Dev           Electronics & Power Systems   \n",
      "11     General Software Dev                   Infotainment System   \n",
      "12     General Software Dev                 Machine Learning & AI   \n",
      "13     General Software Dev  Manufacturing & Process Optimization   \n",
      "14     General Software Dev            Robotics & Control Systems   \n",
      "15     General Software Dev                 Simulation & Modeling   \n",
      "16  Machine Learning & Data           Electronics & Power Systems   \n",
      "17  Machine Learning & Data                 Machine Learning & AI   \n",
      "18  Machine Learning & Data            Robotics & Control Systems   \n",
      "19  Machine Learning & Data                 Simulation & Modeling   \n",
      "20     Testing & Validation                 Machine Learning & AI   \n",
      "21     Testing & Validation  Manufacturing & Process Optimization   \n",
      "22     Testing & Validation            Robotics & Control Systems   \n",
      "23     Testing & Validation                 Simulation & Modeling   \n",
      "\n",
      "    cate_user_count  total_user_count  percentage  \n",
      "0                 1                12    0.083333  \n",
      "1                 2                12    0.166667  \n",
      "2                 3                12    0.250000  \n",
      "3                 2                12    0.166667  \n",
      "4                 3               121    0.024793  \n",
      "5               117               121    0.966942  \n",
      "6                 1               121    0.008264  \n",
      "7                 3               121    0.024793  \n",
      "8                 4               121    0.033058  \n",
      "9                 4               121    0.033058  \n",
      "10               21               429    0.048951  \n",
      "11                1               429    0.002331  \n",
      "12               77               429    0.179487  \n",
      "13               44               429    0.102564  \n",
      "14               21               429    0.048951  \n",
      "15               42               429    0.097902  \n",
      "16                1                34    0.029412  \n",
      "17               32                34    0.941176  \n",
      "18                1                34    0.029412  \n",
      "19                2                34    0.058824  \n",
      "20                1                22    0.045455  \n",
      "21                1                22    0.045455  \n",
      "22                2                22    0.090909  \n",
      "23                2                22    0.090909  \n"
     ]
    }
   ],
   "source": [
    "cate_user_counts = merged_df.groupby(['job_cate', 'cate'])['anon_user_id'].nunique().reset_index(name='cate_user_count')\n",
    "\n",
    "# Calculate the total unique count of users under each job_cate\n",
    "total_user_counts = merged_df.groupby('job_cate')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Merge the two dataframes to calculate the percentage\n",
    "merged_counts = pd.merge(cate_user_counts, total_user_counts, on='job_cate')\n",
    "\n",
    "# Calculate the percentage of users in each cate under each job_cate\n",
    "merged_counts['percentage'] = merged_counts['cate_user_count'] / merged_counts['total_user_count']\n",
    "\n",
    "# Display the final result\n",
    "print(merged_counts[['job_cate', 'cate', 'cate_user_count', 'total_user_count', 'percentage']])\n",
    "\n",
    "merged_counts.to_csv('woven_job_cate_skill_cate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "job cate + common skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  job_cate                            value  count  \\\n",
      "44            Architecture                            Linux      9   \n",
      "72            Architecture     Software Architecture Design      8   \n",
      "1             Architecture                             ADAS      7   \n",
      "73            Architecture             Software Development      7   \n",
      "11            Architecture                              C++      6   \n",
      "...                    ...                              ...    ...   \n",
      "928   Testing & Validation                Agile Methodology      6   \n",
      "990   Testing & Validation  Software Development Life Cycle      6   \n",
      "936   Testing & Validation           Continuous Integration      5   \n",
      "982   Testing & Validation              Root Cause Analysis      5   \n",
      "1003  Testing & Validation      Testing Scenario Management      5   \n",
      "\n",
      "      total_user_count  percentage  \n",
      "44                  12    0.750000  \n",
      "72                  12    0.666667  \n",
      "1                   12    0.583333  \n",
      "73                  12    0.583333  \n",
      "11                  12    0.500000  \n",
      "...                ...         ...  \n",
      "928                 22    0.272727  \n",
      "990                 22    0.272727  \n",
      "936                 22    0.227273  \n",
      "982                 22    0.227273  \n",
      "1003                22    0.227273  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Group by 'job_cate' and 'value' to get the unique count of 'anon_user_id'\n",
    "item_counts = merged_df.groupby(['job_cate', 'value'])['anon_user_id'].nunique().reset_index(name='count')\n",
    "\n",
    "# Step 2: Calculate the total count of 'anon_user_id' for each 'job_cate'\n",
    "total_user_counts = merged_df.groupby('job_cate')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 3: Calculate the percentage for each 'value' under each 'job_cate'\n",
    "merged_counts = pd.merge(item_counts, total_user_counts, on='job_cate')\n",
    "\n",
    "merged_counts['percentage'] = merged_counts['count'] / merged_counts['total_user_count']\n",
    "\n",
    "# Step 4: Sort by 'job_cate' and 'count' (descending), and keep the top N items for each 'job_cate'\n",
    "most_frequent_items = merged_counts.sort_values(['job_cate', 'count'], ascending=[True, False])\n",
    "\n",
    "# Step 5: Select the top N items for each 'job_cate' (N=20 in this case)\n",
    "top_n_items_per_cate = most_frequent_items.groupby('job_cate').head(20)\n",
    "\n",
    "# Output the result\n",
    "print(top_n_items_per_cate)\n",
    "\n",
    "top_n_items_per_cate.to_csv('woven_jobcate_commonskill.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "country+job_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           country                 job_cate  count  total_user_count  \\\n",
      "0            Japan             Architecture      9               301   \n",
      "1            Japan          Embedded System     80               301   \n",
      "2            Japan     General Software Dev    241               301   \n",
      "3            Japan  Machine Learning & Data     23               301   \n",
      "4            Japan     Testing & Validation     11               301   \n",
      "5              USA             Architecture      3               194   \n",
      "6              USA          Embedded System     38               194   \n",
      "7              USA     General Software Dev    170               194   \n",
      "8              USA  Machine Learning & Data      9               194   \n",
      "9              USA     Testing & Validation     11               194   \n",
      "10  United Kingdom          Embedded System      3                20   \n",
      "11  United Kingdom     General Software Dev     18                20   \n",
      "12  United Kingdom  Machine Learning & Data      2                20   \n",
      "\n",
      "    percentage  \n",
      "0     0.029900  \n",
      "1     0.265781  \n",
      "2     0.800664  \n",
      "3     0.076412  \n",
      "4     0.036545  \n",
      "5     0.015464  \n",
      "6     0.195876  \n",
      "7     0.876289  \n",
      "8     0.046392  \n",
      "9     0.056701  \n",
      "10    0.150000  \n",
      "11    0.900000  \n",
      "12    0.100000  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Group by 'job_cate' and 'value' to get the unique count of 'anon_user_id'\n",
    "\n",
    "\n",
    "item_counts = merged_df.groupby(['country', 'job_cate'])['anon_user_id'].nunique().reset_index(name='count')\n",
    "\n",
    "# Step 2: Calculate the total count of 'anon_user_id' for each 'job_cate'\n",
    "total_user_counts = merged_df.groupby('country')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 3: Calculate the percentage for each 'value' under each 'job_cate'\n",
    "merged_counts = pd.merge(item_counts, total_user_counts, on='country')\n",
    "\n",
    "merged_counts['percentage'] = merged_counts['count'] / merged_counts['total_user_count']\n",
    "\n",
    "print(merged_counts)\n",
    "\n",
    "merged_counts.to_csv('woven_country_jobcate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "employee type + job_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      employee_type                 job_cate  count  \\\n",
      "0                        Fixed Term     General Software Dev      2   \n",
      "1                         Full Time             Architecture      8   \n",
      "2                         Full Time          Embedded System     70   \n",
      "3                         Full Time     General Software Dev    263   \n",
      "4                         Full Time  Machine Learning & Data     22   \n",
      "5                         Full Time     Testing & Validation     14   \n",
      "6                          Secondee             Architecture      4   \n",
      "7                          Secondee          Embedded System     49   \n",
      "8                          Secondee     General Software Dev    158   \n",
      "9                          Secondee  Machine Learning & Data     12   \n",
      "10                         Secondee     Testing & Validation      8   \n",
      "11              Secondee Concurrent          Embedded System      1   \n",
      "12              Secondee Concurrent     General Software Dev      2   \n",
      "13  Secondee International Assignee     General Software Dev      1   \n",
      "14                Secondee Outbound          Embedded System      1   \n",
      "15                Secondee Outbound     General Software Dev      1   \n",
      "16                 Secondee Trainee     General Software Dev      2   \n",
      "\n",
      "    total_user_count  percentage  \n",
      "0                  2    1.000000  \n",
      "1                312    0.025641  \n",
      "2                312    0.224359  \n",
      "3                312    0.842949  \n",
      "4                312    0.070513  \n",
      "5                312    0.044872  \n",
      "6                195    0.020513  \n",
      "7                195    0.251282  \n",
      "8                195    0.810256  \n",
      "9                195    0.061538  \n",
      "10               195    0.041026  \n",
      "11                 2    0.500000  \n",
      "12                 2    1.000000  \n",
      "13                 1    1.000000  \n",
      "14                 1    1.000000  \n",
      "15                 1    1.000000  \n",
      "16                 2    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Group by 'job_cate' and 'value' to get the unique count of 'anon_user_id'\n",
    "item_counts = merged_df.groupby(['employee_type', 'job_cate'])['anon_user_id'].nunique().reset_index(name='count')\n",
    "\n",
    "# Step 2: Calculate the total count of 'anon_user_id' for each 'job_cate'\n",
    "total_user_counts = merged_df.groupby('employee_type')['anon_user_id'].nunique().reset_index(name='total_user_count')\n",
    "\n",
    "# Step 3: Calculate the percentage for each 'value' under each 'job_cate'\n",
    "merged_counts = pd.merge(item_counts, total_user_counts, on='employee_type')\n",
    "\n",
    "merged_counts['percentage'] = merged_counts['count'] / merged_counts['total_user_count']\n",
    "\n",
    "print(merged_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar people we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'created_by' with at least 4 covered values: 0\n"
     ]
    }
   ],
   "source": [
    "max_experience = achievement_1.groupby('anon_user_id')['year_of_experiences'].max().reset_index()\n",
    "\n",
    "# Step 2: Filter for max year_of_experiences >= 5\n",
    "filtered_experience = max_experience[max_experience['year_of_experiences'] >= 5]\n",
    "\n",
    "# Step 3: Get the unique list of 'created_by'\n",
    "unique_created_by = filtered_experience['anon_user_id'].unique()\n",
    "\n",
    "filtered_df2 = filtered_concatenated_df[filtered_concatenated_df['anon_user_id'].isin(unique_created_by)]\n",
    "\n",
    "# Step 2: List of values to check for\n",
    "target_values = [\n",
    "    'Embedded System Configuration', \n",
    "    'Sensor Systems', \n",
    "    'Microcontrollers', \n",
    "    'Real-Time Control Firmwares', \n",
    "    'Process Failure Mode And Effects Analysis (PFMEA)', \n",
    "    'Design Failure Mode And Effects Analysis'\n",
    "]\n",
    "\n",
    "# Step 3: Filter rows where 'value' is in the target values\n",
    "filtered_values_df = filtered_df2[filtered_df2['value'].isin(target_values)]\n",
    "\n",
    "# Step 4: Group by 'created_by' and count the number of unique target values for each person\n",
    "value_counts = filtered_values_df.groupby('anon_user_id')['value'].nunique().reset_index()\n",
    "\n",
    "# Step 5: Filter for those who have covered at least 4 values\n",
    "sufficient_coverage = value_counts[value_counts['value'] >= 3]\n",
    "\n",
    "# Step 6: Get the count of created_by who meet the criteria\n",
    "count_created_by = sufficient_coverage['anon_user_id'].nunique()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of 'created_by' with at least 4 covered values: {count_created_by}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "industry analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv('filtered_engineer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_company_name\n",
       "cariad              245\n",
       "cruise               47\n",
       "horizon robotics     13\n",
       "momenta               6\n",
       "motional             29\n",
       "tesla               108\n",
       "waymo                23\n",
       "wayve                 7\n",
       "zoox                 44\n",
       "Name: full_name, dtype: int64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.groupby(['job_company_name'])['full_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create job level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_job_level(job_title):\n",
    "    job_title = job_title.lower()\n",
    "    if 'manager' in job_title:\n",
    "        return 'L9'\n",
    "    elif 'tech lead' in job_title:\n",
    "        return 'L9'\n",
    "    elif 'senior staff' in job_title:\n",
    "        return 'L7'\n",
    "    elif 'staff' in job_title:\n",
    "        return 'L6'\n",
    "    elif 'principal' in job_title:\n",
    "        return 'L8'\n",
    "    elif 'senior' in job_title:\n",
    "        return 'L5'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the functions to create new columns\n",
    "\n",
    "filtered_df['job_level'] = filtered_df['job_title'].apply(categorize_job_level)\n",
    "\n",
    "def categorize_job_level2(job_level):\n",
    "    #job_level = job_level.lower()\n",
    "    if job_level == 'L5' or job_level =='L6':\n",
    "        return 'L5-6'\n",
    "    elif job_level == 'L7' or job_level =='L8':\n",
    "        return 'L7-8'\n",
    "    elif job_level == 'Other':\n",
    "        return 'General'\n",
    "    else:\n",
    "        return 'L9'\n",
    "\n",
    "# Apply the functions to create new columns\n",
    "\n",
    "filtered_df['job_level2'] = filtered_df['job_level'].apply(categorize_job_level2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "job cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_keywords = ['data scien', 'data anal', 'machine lear', 'algorithm']\n",
    "testing_keywords = ['test', 'validation', 'quality', 'qa']\n",
    "safety_keywords = ['safety', 'reliability']\n",
    "function_concept_keywords = ['function and concept']\n",
    "architect_keywords = ['architect']\n",
    "\n",
    "embedded_system_related_keywords = [\n",
    "    \"Embedded\", \"Sensor\", \"Sensors\", \"Sensorset\", \"Electronic Control Units\", \n",
    "    \"Microcontroller\", \"Microprocessor\", \"Actuator\", \"Radar\", \n",
    "    \"LiDAR\", \"Ultrasonic Sensor\", \"Camera\", \"Real-time operating system\",\n",
    "    \"Communication Interface\", \"CAN Bus\", \"Ethernet\", \n",
    "    \"FlexRay\", \"Power Management\", \"control system\",\"ecu\",\"Controller Area Network\",\"rtos\"\n",
    "]\n",
    "\n",
    "embedded_system_related_keywords = [keyword.lower() for keyword in embedded_system_related_keywords]\n",
    "\n",
    "# Function to categorize job titles based on keywords\n",
    "def categorize_job(row):\n",
    "    job_title = row['job_title'].lower()\n",
    "    summary_achievements = row['summary_achievements'].lower()  # Join list to string\n",
    "    skills = ' '.join(row['skills']).lower()  # Join list to string\n",
    "\n",
    "    # Machine Learning and Data related\n",
    "    if any(keyword in job_title for keyword in ml_data_keywords):\n",
    "        return 'Machine Learning & Data'\n",
    "    \n",
    "    # Testing & Validation\n",
    "    elif any(keyword in job_title for keyword in testing_keywords):\n",
    "        return 'Testing & Validation'\n",
    "    \n",
    "    # Safety\n",
    "    elif any(keyword in job_title for keyword in safety_keywords):\n",
    "        return 'Functional Safety'\n",
    "    \n",
    "    # Function and concept engineer\n",
    "    elif any(keyword in job_title for keyword in function_concept_keywords):\n",
    "        return 'Function & concept'\n",
    "    \n",
    "    # Systems Architecture related\n",
    "    elif any(keyword in job_title for keyword in architect_keywords):\n",
    "        return 'Architecture'\n",
    "    \n",
    "    # Embedded System related\n",
    "    elif any(keyword in summary_achievements for keyword in embedded_system_related_keywords) or \\\n",
    "         any(keyword in skills for keyword in embedded_system_related_keywords):\n",
    "        return 'Embedded System'\n",
    "    \n",
    "    # General Software Engineer (default)\n",
    "    else:\n",
    "        return 'General Software Dev'\n",
    "\n",
    "# Apply the function to create the new column\n",
    "filtered_df['job_cate'] = filtered_df.apply(categorize_job, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "core skill count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [job_cate, job_level2, average_unique_mapped_count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "exploded_df = filtered_df.explode('mapped_result')\n",
    "\n",
    "# Step 2: Merge with skill_cate to get the category (cate) for each value in `mapped_result`\n",
    "market_merged_df = exploded_df.merge(skill_cate, left_on='mapped_result', right_on='value', how='inner')\n",
    "unique_counts = market_merged_df.groupby(['job_cate', 'job_level2', 'full_name'])['mapped_result'].nunique().reset_index(name='unique_mapped_count')\n",
    "\n",
    "# Step 2: Calculate the average unique count for each combination of 'job_cate' and 'job_level2'\n",
    "average_unique_counts = unique_counts.groupby(['job_cate', 'job_level2'])['unique_mapped_count'].mean().reset_index(name='average_unique_mapped_count')\n",
    "\n",
    "# Display the result\n",
    "print(average_unique_counts)\n",
    "average_unique_counts.to_csv('average_core_skill.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most common skill under each job category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 job_cate                                      mapped_result  \\\n",
      "0            Architecture  ['ADAS algorithms', 'ADAS', 'Pipeline Developm...   \n",
      "1            Architecture         ['ADAS algorithms', 'HIL Testing', 'ADAS']   \n",
      "2            Architecture  ['ADAS', 'Digital Signal Processing', 'ADAS al...   \n",
      "3            Architecture  ['Android Applications', 'JavaScript (Programm...   \n",
      "4            Architecture  ['Application Lifecycle Management (ALM) Softw...   \n",
      "..                    ...                                                ...   \n",
      "438  Testing & Validation  ['C++', 'Eclipse (Software)', 'Semantic HTML',...   \n",
      "439  Testing & Validation  ['C++', 'Equipment Design', 'Industrial Roboti...   \n",
      "440  Testing & Validation  ['C++', 'Lauterbach', 'Eclipse (Software)', 'G...   \n",
      "441  Testing & Validation  ['C++', 'Microsoft SQL Servers', 'Embedded Sys...   \n",
      "442  Testing & Validation  ['C++', 'Semantic HTML', '3D Modeling', 'Safet...   \n",
      "\n",
      "     count  total_user_count  percentage  \n",
      "0        1                18    0.055556  \n",
      "1        1                18    0.055556  \n",
      "2        1                18    0.055556  \n",
      "3        1                18    0.055556  \n",
      "4        1                18    0.055556  \n",
      "..     ...               ...         ...  \n",
      "438      1                75    0.013333  \n",
      "439      1                75    0.013333  \n",
      "440      1                75    0.013333  \n",
      "441      1                75    0.013333  \n",
      "442      1                75    0.013333  \n",
      "\n",
      "[164 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "item_counts = exploded_df.groupby(['job_cate', 'mapped_result'])['full_name'].nunique().reset_index(name='count')\n",
    "\n",
    "# Step 3: Find the most frequent items for each `job_cate`\n",
    "# Sort by `job_cate` and `count` (descending), then keep only the top item for each `job_cate`\n",
    "most_frequent_items = item_counts.sort_values(['job_cate', 'count'], ascending=[True, False])\n",
    "total_user_counts = exploded_df.groupby('job_cate')['full_name'].nunique().reset_index(name='total_user_count')\n",
    "merged_counts = pd.merge(most_frequent_items, total_user_counts, on='job_cate')\n",
    "merged_counts['percentage'] = merged_counts['count'] / merged_counts['total_user_count']\n",
    "# Optionally: You can keep the top N items for each job_cate using the following code\n",
    "top_n_items_per_cate = merged_counts.groupby('job_cate').head(30)  # Change `1` to `N` for top N\n",
    "\n",
    "# Output the result\n",
    "print(top_n_items_per_cate)\n",
    "top_n_items_per_cate.to_csv('market_job_cate_skills.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                job_cate job_level2  degree_category  degree_count  \\\n",
      "0           Architecture    General         bachelor             1   \n",
      "1           Architecture    General  below undergrad             4   \n",
      "2           Architecture    General           master             2   \n",
      "3           Architecture    General              phd             2   \n",
      "4           Architecture       L5-6         bachelor             2   \n",
      "..                   ...        ...              ...           ...   \n",
      "63  Testing & Validation       L5-6           master            13   \n",
      "64  Testing & Validation       L5-6              phd             1   \n",
      "65  Testing & Validation       L7-8           master             1   \n",
      "66  Testing & Validation         L9         bachelor             1   \n",
      "67  Testing & Validation         L9           master             1   \n",
      "\n",
      "    total_count  %_of_degrees  \n",
      "0             9     11.111111  \n",
      "1             9     44.444444  \n",
      "2             9     22.222222  \n",
      "3             9     22.222222  \n",
      "4             8     25.000000  \n",
      "..          ...           ...  \n",
      "63           24     54.166667  \n",
      "64           24      4.166667  \n",
      "65            1    100.000000  \n",
      "66            2     50.000000  \n",
      "67            2     50.000000  \n",
      "\n",
      "[68 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_df['education'] = filtered_df['education'].apply(ast.literal_eval)\n",
    "grouped = filtered_df.groupby('job_cate')\n",
    "\n",
    "# Step 2: Define a function that performs the analysis on each group\n",
    "def process_group(df):\n",
    "    # Explode the 'education' column\n",
    "    education_exploded = df.explode('education').reset_index(drop=True)\n",
    "    \n",
    "    # Normalize the nested JSON structure to flatten it into columns\n",
    "    education_df = pd.json_normalize(education_exploded['education'])\n",
    "    \n",
    "    # Add the 'full_name' back to the normalized DataFrame\n",
    "    education_df['full_name'] = education_exploded['full_name']\n",
    "    \n",
    "    # Convert 'end_date' to numeric, handling cases where it might be missing or None\n",
    "    education_df['end_date'] = pd.to_numeric(education_df['end_date'], errors='coerce')\n",
    "    \n",
    "    # Sort by 'full_name' and 'end_date' to get the most recent entry per person\n",
    "    education_df_sorted = education_df.sort_values(by=['full_name', 'end_date'], ascending=[True, False])\n",
    "    \n",
    "    # Drop duplicates keeping the most recent entry per person\n",
    "    education_most_recent = education_df_sorted.drop_duplicates(subset=['full_name'], keep='first')\n",
    "    \n",
    "    # Select the relevant columns: 'degrees', 'school.name', and 'full_name'\n",
    "    education_filtered = education_most_recent[['full_name', 'degrees', 'school.name']]\n",
    "    \n",
    "    return education_filtered\n",
    "\n",
    "# Step 3: Apply the function to each group and concatenate the results\n",
    "education_per_job_cate = grouped.apply(process_group).reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "#print(education_per_job_cate)\n",
    "\n",
    "\n",
    "def categorize_degree(degree):\n",
    "    # Convert degree to lowercase for easier matching\n",
    "    degree = str(degree).lower()\n",
    "    \n",
    "    if 'master' in degree:\n",
    "        return 'master'\n",
    "    elif 'bachelor' in degree:\n",
    "        return 'bachelor'\n",
    "    elif 'doctor' in degree or 'phd' in degree:\n",
    "        return 'phd'\n",
    "    else:\n",
    "        return 'below undergrad'\n",
    "\n",
    "# Apply the categorize_degree function to the 'degrees' column\n",
    "education_per_job_cate['degree_category'] = education_per_job_cate['degrees'].apply(categorize_degree)\n",
    "\n",
    "\n",
    "\n",
    "# Merge filtered_df and education_per_job_cate on 'full_name'\n",
    "merged_df = filtered_df.merge(education_per_job_cate[['full_name', 'degree_category']], on='full_name', how='left')\n",
    "\n",
    "# Step 1: Calculate the count of each degree_category within each job_cate and job_level2\n",
    "degree_count_per_job_cate = merged_df.groupby(['job_cate', 'job_level2', 'degree_category']).size().reset_index(name='degree_count')\n",
    "\n",
    "# Step 2: Calculate the total count of all degrees for each job_cate and job_level2\n",
    "total_count_per_job_cate = merged_df.groupby(['job_cate', 'job_level2']).size().reset_index(name='total_count')\n",
    "\n",
    "# Step 3: Merge the two DataFrames to calculate the percentage\n",
    "merged_counts = pd.merge(degree_count_per_job_cate, total_count_per_job_cate, on=['job_cate', 'job_level2'])\n",
    "\n",
    "# Step 4: Calculate the percentage of each degree_category within job_cate and job_level2\n",
    "merged_counts['%_of_degrees'] = (merged_counts['degree_count'] / merged_counts['total_count']) * 100\n",
    "\n",
    "# Display the result\n",
    "print(merged_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "company+job cate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Group the data by 'job_company_name' and 'job_cate' to get the counts\n",
    "job_cate_counts = filtered_df.groupby(['job_company_name', 'job_cate']).size().reset_index(name='count')\n",
    "\n",
    "# Step 2: Calculate the total count per company to get percentages\n",
    "job_cate_counts['total_count'] = job_cate_counts.groupby('job_company_name')['count'].transform('sum')\n",
    "job_cate_counts['percentage'] = (job_cate_counts['count'] / job_cate_counts['total_count']) * 100\n",
    "\n",
    "# Step 3: Plot pie charts for each job_company_name\n",
    "companies = job_cate_counts['job_company_name'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
